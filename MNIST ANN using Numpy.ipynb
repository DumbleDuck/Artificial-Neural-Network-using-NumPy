{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b134b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73dada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights\n",
    "def init_params(layer_size):\n",
    "    params={}\n",
    "    for i in range(1,len(layer_size)):\n",
    "        params[\"W\"+ str(i)]= np.random.randn(layer_size[i], layer_size[i-1])* .01\n",
    "        #params[\"W\"+ str(i)]= np.random.randint(low = -2, high= 4, size=(layer_size[i], layer_size[i-1]))*.1  # For Debugging\n",
    "        params[\"b\"+ str(i)]= np.zeros((layer_size[i],1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952bfe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot Encoder\n",
    "def one_hot(Y, num_channels):\n",
    "    m= np.shape(Y)[0]\n",
    "    one_h= np.zeros((num_channels,m))\n",
    "    index=0\n",
    "    for i in Y:\n",
    "        one_h[i, index]=1\n",
    "        index= index+1\n",
    "    \n",
    "    return one_h  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(Z):\n",
    "    sig= 1/ (1+ np.exp(Z))\n",
    "    \n",
    "    return sig    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37dc491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def relu(Z):\n",
    "    relu= np.maximum(0,Z)\n",
    "    \n",
    "    return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324469c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(Z):\n",
    "    soft= np.exp(Z)/ (np.sum(np.exp(Z), axis=0))\n",
    "    \n",
    "    return soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ab6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation\n",
    "def activation(Z, activation):\n",
    "    if activation== \"relu\":\n",
    "        A= relu(Z)\n",
    "    elif activation== \"sigmoid\":\n",
    "        A= sigmoid(Z)\n",
    "    elif activation== \"softmax\":\n",
    "        A=softmax(Z)\n",
    "    \n",
    "    return A   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Backprop\n",
    "def softmax_back(dA, caches, layer_num, Y_train):\n",
    "    al, _, _, _ = caches[layer_num-1]\n",
    "    sofback= al-Y_train\n",
    "    \n",
    "    return sofback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu backprop\n",
    "def relu_back(dA, caches, layer_num):\n",
    "    _, _, _, Zl= caches[layer_num-1]\n",
    "    relback= dA* np.heaviside(Zl, 0)\n",
    "    \n",
    "    return relback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c88b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation \n",
    "def forward_prop_single(A_prev, W, b):\n",
    "    Z= np.dot(W, A_prev) + b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation final\n",
    "def forw_prop(X_train, layer, params):\n",
    "    \n",
    "    layer_dim= layer[\"Dim\"]\n",
    "    layer_activs= layer[\"Activs\"]\n",
    "    \n",
    "    L= len(layer_dim)\n",
    "    #params= init_params(layer_dim) \n",
    "    caches=[]\n",
    "    A= X_train\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev=A\n",
    "        W= params[\"W\" + str(l)]\n",
    "        b= params[\"b\" + str(l)]\n",
    "        Z= forward_prop_single(A_prev, W, b)\n",
    "        A= activation(Z, layer_activs[l-1])\n",
    "        cache= (A, W, b, Z)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    return caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc57e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Cost Function\n",
    "def cost(Y_train, Y, params, lamda):\n",
    "    m= np.shape(Y_train)[1]\n",
    "    loss= -np.sum(np.multiply(Y_train, np.log(Y)), axis= 0)\n",
    "    c= (1/m)*np.sum(loss)\n",
    "    c= np.squeeze(c)\n",
    "    reg=0\n",
    "    for i in range(len(params)//2):\n",
    "        reg= reg+ np.sum(np.square(params[\"W\" + str(i+1)]))    \n",
    "    reg=reg* (lamda/(2*m))\n",
    "    c=c+reg\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbef5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single backprop\n",
    "def back_prop_single(dA, caches, layer_num, activation, X_train, Y_train, lamda):\n",
    "    if activation== \"relu\":\n",
    "        dZ= relu_back(dA, caches, layer_num)\n",
    "    elif activation==\"softmax\":\n",
    "        dZ= softmax_back(dA, caches, layer_num, Y_train)\n",
    "    m= np.shape(dA)[1]    \n",
    "    _, Wl, _, _= caches[layer_num-1]\n",
    "    \n",
    "    if layer_num==1:\n",
    "        al_1= X_train\n",
    "    else:\n",
    "        al_1, _, _, _ = caches[layer_num-2]\n",
    "        \n",
    "    dW= (1/m)*np.dot(dZ ,np.transpose(al_1)) + (lamda/m)*Wl\n",
    "    db= (1/m)* np.sum(dZ, axis=1, keepdims=True)\n",
    "    dAl_1= np.dot(np.transpose(Wl), dZ)\n",
    "    \n",
    "    return dW, db, dAl_1, dZ    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a18d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N layer backprop\n",
    "def back_prop(Y_train, caches, layer, X_train, lamda):\n",
    "    layer_dim= layer[\"Dim\"]\n",
    "    layer_activs= layer[\"Activs\"]\n",
    "    L= len(caches)\n",
    "    A_L, _, _, _= caches[L-1]\n",
    "    dA= -(Y_train/A_L) \n",
    "    grads={}\n",
    "    grads[\"dA\"+ str(L)]= dA\n",
    "    \n",
    "    for l in range(L, 0, -1):\n",
    "        dW, db, dA, dZ= back_prop_single(dA, caches, l, layer_activs[l-1], X_train, Y_train, lamda)\n",
    "        grads[\"dW\" + str(l)]= dW\n",
    "        grads[\"db\" + str(l)]= db\n",
    "        grads[\"dA\" + str(l-1)]= dA\n",
    "        grads[\"dZ\" + str(l)]= dZ\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "def upd_params(grads, params, l_rate):\n",
    "    parameters=params.copy()\n",
    "    L=len(parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)]= parameters[\"W\"+str(l+1)] -l_rate*grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\"+str(l+1)]= parameters[\"b\"+str(l+1)] -l_rate*grads[\"db\"+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def acc(X_dev, Y_dev, params, layer):\n",
    "    dev_cache= forw_prop(X_dev, layer, params)\n",
    "    preds, _, _, _= dev_cache[len(dev_cache)-1]\n",
    "    preds= np.argmax(preds, axis=0)\n",
    "    correct= np.sum(preds== Y_dev)\n",
    "    accurate= correct/ np.shape(Y_dev)[0]\n",
    "    \n",
    "    return preds,accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4eb3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MAIN\n",
    "\n",
    "# For Debugging \n",
    "'''\n",
    "X_train= np.array([[1, 2, 1], [2, 1, 3], [3, 1, 3], [4, 3, 3]])\n",
    "Y_train= np.array([[1, 0, 0], [0, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "\n",
    "\n",
    "Y_train= np.array([[0,1,0,0,0,1,0], [0,0,1,0,1,0,0], [0,0,0,1,0,0,1], [1,0,0,0,0,0,0]])\n",
    "X_train= np.array([[10,80,11,49,15,55,32], [20,70,19,60, 85,16, 43], [30,60,15,70,9,36,69], [40,50,13,38,10,48,24]])\n",
    "X_mean= np.mean(X_train, axis= 1, keepdims=True)\n",
    "X_std= np.std(X_train, axis=1, keepdims=True) \n",
    "X_train= (X_train-X_mean)/X_std\n",
    "m_train= 7\n",
    "'''\n",
    "# Import and one_hot encoding \n",
    "data= pd.read_csv(r'E:\\My files\\Python\\DL\\Datasets\\MNIST\\train.csv')\n",
    "data= np.array(data)\n",
    "data= np.transpose(data)\n",
    "m= np.shape(data)[1]             #Total examples\n",
    "m_train= 41000                   #Number of examples to be used for training\n",
    "\n",
    "X_train= data[1:785, 0:m_train]\n",
    "X_train= X_train/255\n",
    "\n",
    "Y_train= data[0, 0:m_train]\n",
    "Y_train_acc= Y_train\n",
    "Y_train= one_hot(Y_train, 10)\n",
    "\n",
    "X_dev= data[1:785, m_train:m]\n",
    "X_dev= X_dev/255\n",
    "Y_dev= data[0, m_train:m]\n",
    "\n",
    "\n",
    "#Layer Data\n",
    "layer_data={ \"Dim\": [784,300,10], \"Activs\": [\"relu\", \"softmax\"]}\n",
    "\n",
    "\n",
    "#Hyperparameters\n",
    "epochs= 30                         #Iterations\n",
    "rate = 0.5                         #learning rate\n",
    "batch_size= 512                    #Mini-batch size\n",
    "mini_batch= m_train // batch_size  #Number of mini-batches\n",
    "lamda= 0                           #Regularization parameter\n",
    "\n",
    "#Initialize parameters\n",
    "params= init_params(layer_data[\"Dim\"])\n",
    "cost_history=[]\n",
    "dev_acc=[]\n",
    "train_acc=[]\n",
    "print(params)\n",
    "\n",
    "#Training the model\n",
    "for ep in range(epochs):\n",
    "   \n",
    "    print(\"Epoch \"+str(ep+1))\n",
    "    \n",
    "    #Adaptive Learning Rate\n",
    "    if (ep+1)%26 ==0:\n",
    "        rate = rate/2\n",
    "            \n",
    "    for batch in range(mini_batch):\n",
    "        start_index= batch_size*batch\n",
    "        end_index= min((batch+1)*batch_size, m_train)\n",
    "       \n",
    "        X_train_batch = X_train[:, start_index: end_index]\n",
    "        Y_train_batch = Y_train[:, start_index: end_index]\n",
    "        \n",
    "   \n",
    "        # Forward propagation\n",
    "        for_cache= forw_prop(X_train_batch, layer_data, params)\n",
    "        #for i in range(len(for_cache)):\n",
    "            #print(\"A\"+str(i+1),\",W\"+str(i+1), \",b\"+str(i+1),\",Z\"+str(i+1),\"\\n\", for_cache[i], '\\n')\n",
    "        Y,_,_,_= for_cache[-1]\n",
    "\n",
    "        \n",
    "        # Cost\n",
    "        cost_forward= cost(Y_train_batch, Y, params, lamda)\n",
    "        print(\"Cost:\", cost_forward)\n",
    "        cost_history.append(cost_forward)\n",
    "\n",
    "        \n",
    "        # Back Propagation\n",
    "        gradients= back_prop(Y_train_batch, for_cache, layer_data, X_train_batch, lamda)\n",
    "    \n",
    "        #for i in  range(len(for_cache), 0, -1):\n",
    "            #print(\"dA\"+str(i)+\": \" ,gradients[\"dA\"+str(i)], \"\\n\")\n",
    "            #print(\"dZ\"+str(i)+\": \" ,gradients[\"dZ\"+str(i)], \"\\n\")\n",
    "            #print(\"dW\"+str(i)+\": \" ,gradients[\"dW\"+str(i)], \"\\n\")\n",
    "            #print(\"db\"+str(i)+\": \" ,gradients[\"db\"+str(i)], \"\\n\")\n",
    "\n",
    "        # Update Parameters\n",
    "        params = upd_params(gradients, params, rate)\n",
    "        #print(params.items())\n",
    "        \n",
    "        \n",
    "        _, accuracy= acc(X_dev, Y_dev, params, layer_data)\n",
    "        dev_acc.append(accuracy)\n",
    "        print(\"Dev Accuracy:\", accuracy)\n",
    "        \n",
    "        _, accuracy= acc(X_train, Y_train_acc, params, layer_data)\n",
    "        train_acc.append(accuracy)\n",
    "        print(\"Train Accuracy:\", accuracy, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading exisiting models \n",
    "\n",
    "with open('97dot6_MNIST_300units_30ep_512mbgd_0lamda.json', 'rb') as fp:\n",
    "    params=pickle.load(fp)\n",
    "\n",
    "print(params)\n",
    "\n",
    "_, accuracy= acc(X_dev, Y_dev, params, layer_data)\n",
    "print(\"Dev Accuracy:\", accuracy)\n",
    "\n",
    "_, accuracy= acc(X_train, Y_train_acc, params, layer_data)\n",
    "print(\"Train Accuracy:\", accuracy, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting cost and training accuracy over iterations\n",
    "%matplotlib inline\n",
    "x_ticks= [i for i in range(0, len(cost_history), 1000)]\n",
    "plt.plot(cost_history)\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.plot(train_acc)\n",
    "plt.plot(dev_acc)\n",
    "plt.savefig('image_4.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae243f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading random example from dev set and predicting it\n",
    "eg= random.randint(0, m-m_train)\n",
    "plt.imshow(np.reshape(X_dev[:, eg], (28,28)), interpolation='gaussian')\n",
    "plt.show()\n",
    "example= np.reshape(X_dev[:, eg], (784,1))\n",
    "hehe = forw_prop(example, layer_data, params)\n",
    "prediction, _, _, _= hehe[len(hehe)-1]\n",
    "prediction= np.squeeze(np.argmax(prediction, axis=0))\n",
    "print(\"That's\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading random Test set examples to evaluate performance\n",
    "X_test= np.array(pd.read_csv(r'E:\\My files\\Python\\DL\\Datasets\\MNIST\\test.csv'))\n",
    "X_test= np.transpose(X_test)\n",
    "X_test=X_test/255\n",
    "\n",
    "eg= random.randint(0, np.shape(X_test)[1])\n",
    "\n",
    "plt.imshow(np.reshape(X_test[:, eg], (28,28)), interpolation='gaussian')\n",
    "plt.show()\n",
    "example= np.reshape(X_test[:, eg], (784,1))\n",
    "hehe = forw_prop(example, layer_data, params)\n",
    "prediction, _, _, _= hehe[len(hehe)-1]\n",
    "prediction= np.squeeze(np.argmax(prediction, axis=0))\n",
    "print(\"That's\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382e085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving trained weights\n",
    "print(params)\n",
    "with open('filename.json', 'wb') as fp:\n",
    "    pickle.dump(params, fp)\n",
    "    \n",
    "with open('filename.json', 'rb') as fp:\n",
    "    data=pickle.load(fp)\n",
    "\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
